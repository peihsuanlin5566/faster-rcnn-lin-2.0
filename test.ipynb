{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datanumber: 5011\n"
     ]
    }
   ],
   "source": [
    "from pro.data.vocdata import VocDataset\n",
    "label_dir = 'pro/data/dataset/VOCdevkit/VOC2007/Annotations'\n",
    "image_dir = 'pro/data/dataset/VOCdevkit/VOC2007/JPEGImages'\n",
    "voc_dataset = VocDataset(label_dir, image_dir)\n",
    "print('datanumber: {}'.format(voc_dataset.__len__()))\n",
    "# next(iter(voc_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch number of train_data: 12,val_data: 16\n"
     ]
    }
   ],
   "source": [
    "from pro.data.vocdata import VocDataloader\n",
    "voc_dataloader= VocDataloader(voc_dataset)\n",
    "train_dataloader, val_dataloader = voc_dataloader(0.7,batch_size_train=3,sample=100)\n",
    "\n",
    "print('batch number of train_data: {},val_data: {}'.format(train_dataloader.__len__(),val_dataloader.__len__()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'diningtable', 1: 'chair', 2: 'horse', 3: 'person', 4: 'tvmonitor', 5: 'bird', 6: 'cow', 7: 'dog', 8: 'bottle', 9: 'pottedplant', 10: 'aeroplane', 11: 'car', 12: 'cat', 13: 'sheep', 14: 'bicycle', 15: 'sofa', 16: 'boat', 17: 'train', 18: 'motorbike', 19: 'bus'}\n"
     ]
    }
   ],
   "source": [
    "from pro.model.model import FasterRcnn\n",
    "from pro.util.utils import get_class_dict\n",
    "\n",
    "class_name_dict = get_class_dict(label_dir)\n",
    "print(class_name_dict)\n",
    "fasterrcnn_cls = FasterRcnn(class_name_dict, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fasterrcnn_cls.train(train_dataloader, num_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:19<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mAP over 16 images: 0.000\n",
      "images output in /Users/hayashi/Documents/code/20221130_fastrcnn/faster-rcnn-lin-2.0/detect/test\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pred, target_gt, image \u001b[39m=\u001b[39m fasterrcnn_cls\u001b[39m.\u001b[39meval(val_dataloader, \n\u001b[1;32m      2\u001b[0m             load_from\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39moutput_model/221216-175225/train_ALL_VOC2007.cpu.pt\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[1;32m      3\u001b[0m             plot_result\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "fasterrcnn_cls.eval(val_dataloader, \n",
    "            load_from='output_model/221216-175225/train_ALL_VOC2007.cpu.pt', \n",
    "            plot_result=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pro.util.utils import plot_results\n",
    "plot_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "metric  = MeanAveragePrecision()\n",
    "metric.update(pred, target_gt)\n",
    "mAP = metric.compute()['map']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mAP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boxes': tensor([[432.,  86., 460., 147.],\n",
       "         [225.,  62., 399., 274.],\n",
       "         [ 79.,  93., 117., 129.],\n",
       "         [450.,  63., 497., 233.],\n",
       "         [265., 132., 289., 156.],\n",
       "         [  5.,  32.,  84., 146.],\n",
       "         [ 45.,  15., 400., 332.],\n",
       "         [176.,  38., 367., 330.],\n",
       "         [  6.,  11., 230., 308.],\n",
       "         [341.,  79., 397., 250.]], dtype=torch.float64),\n",
       " 'scores': tensor([0.9956, 0.9925, 0.9891, 0.9863, 0.9786, 0.9769, 0.9698, 0.8764, 0.7695,\n",
       "         0.5728], dtype=torch.float64),\n",
       " 'labels': tensor([4., 4., 4., 4., 4., 4., 4., 4., 4., 4.], dtype=torch.float64)}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boxes': tensor([[384., 199., 415., 269.],\n",
       "         [448.,  69., 500., 239.],\n",
       "         [235.,  61., 437., 333.],\n",
       "         [  1.,   5., 466., 333.],\n",
       "         [  1.,  34., 105., 155.],\n",
       "         [433.,  91., 458., 146.],\n",
       "         [344., 117., 390., 208.],\n",
       "         [366., 100., 403., 135.],\n",
       "         [ 86.,  90., 117., 131.],\n",
       "         [245., 106., 268., 146.],\n",
       "         [265., 138., 287., 157.],\n",
       "         [206., 107., 270., 165.]], dtype=torch.float64),\n",
       " 'labels': tensor([8., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.], dtype=torch.float64)}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_gt[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eb97e94ebdf1bc42cd4c39d748bee409433f111db40d9f9118289bcad2f8256e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
